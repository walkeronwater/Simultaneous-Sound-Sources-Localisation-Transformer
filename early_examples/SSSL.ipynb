{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCx2kDoLaAyp"
   },
   "outputs": [],
   "source": [
    "class MyLinearLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        weights = torch.Tensor(size_out, size_in)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        bias = torch.Tensor(size_out)\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_times_x = torch.mm(x, self.weights.t())\n",
    "        return torch.add(w_times_x, self.bias)  # w times x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfkOrVZtVvse"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4s2E6NIVtto",
    "outputId": "24793fb6-5469-4ad0-82f7-64c4b969d551"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# !nvidia-smi\n",
    "# print (torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0Fu0z3JYA6_"
   },
   "source": [
    "# Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yX3vaCO0UUgd"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        # assert debug\n",
    "        assert (\n",
    "          self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # obtain Q K V matrices by linear transformation\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "          energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "          N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "          nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "        ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "          [\n",
    "              TransformerBlock(\n",
    "                  embed_size,\n",
    "                  heads,\n",
    "                  dropout=dropout,\n",
    "                  forward_expansion=forward_expansion,\n",
    "              )\n",
    "              for _ in range(num_layers)\n",
    "          ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "          (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "          out = layer(out, out, out, mask)\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "          embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "        ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "          [\n",
    "              DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "              for _ in range(num_layers)\n",
    "          ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "          x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=1024,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cpu\",\n",
    "        max_length=100,\n",
    "        ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "          src_vocab_size,\n",
    "          embed_size,\n",
    "          num_layers,\n",
    "          heads,\n",
    "          device,\n",
    "          forward_expansion,\n",
    "          dropout,\n",
    "          max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "          trg_vocab_size,\n",
    "          embed_size,\n",
    "          num_layers,\n",
    "          heads,\n",
    "          forward_expansion,\n",
    "          dropout,\n",
    "          device,\n",
    "          max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "          N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhWw-Rccg5zZ",
    "outputId": "71231130-4de6-4b10-fb02-ada50cc8349d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([2, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    # x = torch.tensor(data).to(device)\n",
    "    # shape: (training examples, sequence length)\n",
    "    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
    "    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "    # trg = torch.tensor(fakeY).to(device)\n",
    "\n",
    "    src_pad_idx = 0\n",
    "    trg_pad_idx = 0\n",
    "    src_vocab_size = 10\n",
    "    trg_vocab_size = 10\n",
    "    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "      device\n",
    "    )\n",
    "    out = model(x, trg[:, :-1])\n",
    "    # print(out.shape)\n",
    "    # print(out[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjyHWTSmg-e_",
    "outputId": "27c91278-f997-452a-fd5d-0b5982227a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3F0pIWK4p8-z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5248bDAp-10"
   },
   "source": [
    "# ADRELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHQ5GBzMqCxe"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from .modules import AbstractLocalizationModule, FeatureExtraction, LocalizationOutput\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "from utils import SELLoss\n",
    "\n",
    "\n",
    "class ADRENALINEEncoder(nn.Module):\n",
    "  \"\"\"This class implements the encoder module for a sequence-to-sequence-based sound event localization neural\n",
    "  network. It uses a feature extraction front-end based on convolutional layers, as proposed in\n",
    "      Sharath Adavanne, Archontis Politis, Joonas Nikunen, Tuomas Virtanen: \"Sound Event Localization and Detection\n",
    "          of Overlapping Sources Using Convolutional Recurrent Neural Networks\" (2018)\n",
    "  and implements a standard encoder structure based on gated recurrent units.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "                hparams: argparse.Namespace) -> None:\n",
    "      super(ADRENALINEEncoder, self).__init__()\n",
    "\n",
    "      self.hidden_dim = hparams.hidden_dim\n",
    "      self.num_layers = hparams.num_layers\n",
    "\n",
    "      num_steps_per_chunk = int(2 * hparams.chunk_length / hparams.frame_length)\n",
    "      self.feature_extraction = FeatureExtraction(num_steps_per_chunk,\n",
    "                                                  hparams.num_fft_bins,\n",
    "                                                  dropout_rate=hparams.dropout_rate)\n",
    "\n",
    "      feature_dim = int(hparams.num_fft_bins / 4)\n",
    "\n",
    "      self.initial_state = nn.Parameter(\n",
    "          torch.randn((2 * hparams.num_layers, 1, hparams.hidden_dim), dtype=torch.float32), requires_grad=True\n",
    "      )\n",
    "\n",
    "      self.gru = nn.GRU(feature_dim, hparams.hidden_dim, batch_first=True, bidirectional=True,\n",
    "                        num_layers=hparams.num_layers, dropout=hparams.dropout_rate)\n",
    "\n",
    "  def forward(self,\n",
    "              audio_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "      extracted_features = self.feature_extraction(audio_features)\n",
    "      batch_size = extracted_features.shape[0]\n",
    "\n",
    "      output, hidden = self.gru(extracted_features, self.initial_state.repeat(1, batch_size, 1))\n",
    "\n",
    "      hidden = hidden.view(2, 2, batch_size, -1).permute(0, 2, 3, 1).reshape(self.num_layers, batch_size, 2 * self.hidden_dim)\n",
    "\n",
    "      return output, hidden\n",
    "\n",
    "\n",
    "class ADRENALINEDecoder(nn.Module):\n",
    "  \"\"\"This class implements an attention-based decoder module for a sequence-to-sequence-based sound event localization\n",
    "  neural network. It exploits a standard architecture based on the scaled dot-product for computing attention values\n",
    "  and gated recurrent units as the recurrent part.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "                hparams: argparse.Namespace) -> None:\n",
    "      super(ADRENALINEDecoder, self).__init__()\n",
    "\n",
    "      self.hidden_dim = hparams.hidden_dim\n",
    "      self.num_layers = hparams.num_layers\n",
    "\n",
    "      self.scale_matrix = nn.Linear(2 * hparams.hidden_dim, 2 * hparams.hidden_dim, bias=False)\n",
    "\n",
    "      self.gru = nn.GRU(2 * hparams.hidden_dim + 3 * hparams.max_num_sources, 2 * hparams.hidden_dim,\n",
    "                        batch_first=True, num_layers=hparams.num_layers, dropout=hparams.dropout_rate)\n",
    "\n",
    "      self.localization_output = LocalizationOutput(2 * hparams.hidden_dim, max_num_sources=hparams.max_num_sources)\n",
    "\n",
    "  def forward(self,\n",
    "              source_activity_input: torch.Tensor,\n",
    "              direction_of_arrival_input: torch.Tensor,\n",
    "              hidden: torch.Tensor,\n",
    "              encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "      \"\"\"\n",
    "      :param source_activity_input: input vector indicating source activity at the current time step\n",
    "      :param direction_of_arrival_input: direction-of-arrival input vector at the current time step\n",
    "      :param hidden: decoder hidden state from the previous time step\n",
    "      :param encoder_outputs: all encoder outputs\n",
    "      :return: source_activity_output, direction_of_arrival_output, hidden: corresponding outputs\n",
    "      \"\"\"\n",
    "      batch_size, sequence_length, _ = encoder_outputs.shape\n",
    "\n",
    "      # Compute attention weights via dot product between current hidden state and encoder outputs.\n",
    "      expanded_hidden = hidden[self.num_layers - 1, :].unsqueeze(0).permute(1, 0, 2).repeat(1, sequence_length, 1)\n",
    "\n",
    "      scaled_dot_product = (self.scale_matrix(encoder_outputs) * expanded_hidden).sum(-1) / np.sqrt(self.hidden_dim)\n",
    "      attention_weights = torch.softmax(scaled_dot_product, dim=-1)\n",
    "\n",
    "      context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "      input_with_context = torch.cat((source_activity_input,\n",
    "                                      direction_of_arrival_input.view(batch_size, 1, -1),\n",
    "                                      context_vector), dim=-1)\n",
    "\n",
    "      output, next_hidden = self.gru(input_with_context, hidden)\n",
    "\n",
    "      source_activity_output, direction_of_arrival_output = self.localization_output(output)\n",
    "\n",
    "      return source_activity_output, direction_of_arrival_output, next_hidden, attention_weights\n",
    "\n",
    "\n",
    "class ADRENALINE(AbstractLocalizationModule):\n",
    "  \"\"\"Implementation of the Attention-based Deep REcurrent Network for locALizINg acoustic Events (ADRENALINE).\"\"\"\n",
    "  def __init__(self,\n",
    "                dataset_path: str,\n",
    "                cv_fold_idx: int,\n",
    "                hparams: argparse.Namespace) -> None:\n",
    "      super(ADRENALINE, self).__init__(dataset_path, cv_fold_idx, hparams)\n",
    "\n",
    "      self.max_num_sources = hparams.max_num_sources\n",
    "\n",
    "      self.encoder = ADRENALINEEncoder(hparams)\n",
    "      self.decoder = ADRENALINEDecoder(hparams)\n",
    "\n",
    "  def get_loss_function(self) -> nn.Module:\n",
    "      return SELLoss(self.hparams.max_num_sources, alpha=self.hparams.alpha)\n",
    "\n",
    "  def forward(self,\n",
    "              audio_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, dict]:\n",
    "      batch_size, _, sequence_length, _ = audio_features.shape\n",
    "      device = audio_features.device\n",
    "\n",
    "      source_activity = torch.zeros((batch_size, 1, self.max_num_sources)).to(device)\n",
    "      direction_of_arrival = torch.zeros((batch_size, 1, self.max_num_sources, 2)).to(device)\n",
    "\n",
    "      encoder_outputs, hidden = self.encoder(audio_features)\n",
    "      source_activity_output = torch.zeros((batch_size, sequence_length, self.max_num_sources)).to(device)\n",
    "      direction_of_arrival_output = torch.zeros((batch_size, sequence_length, self.max_num_sources, 2)).to(device)\n",
    "\n",
    "      attention_map = []\n",
    "\n",
    "      for step_idx in range(sequence_length):\n",
    "          source_activity, direction_of_arrival, hidden, attention_weights = self.decoder(\n",
    "              source_activity, direction_of_arrival, hidden, encoder_outputs)\n",
    "\n",
    "          attention_map.append(attention_weights.unsqueeze(-1))\n",
    "\n",
    "          source_activity_output[:, step_idx, :] = source_activity.squeeze()\n",
    "          direction_of_arrival_output[:, step_idx, :, :] = direction_of_arrival.squeeze()\n",
    "\n",
    "      attention_map = torch.cat(attention_map, dim=-1)\n",
    "\n",
    "      meta_data = {\n",
    "          'attention_map': attention_map\n",
    "      }\n",
    "\n",
    "      return source_activity_output, direction_of_arrival_output, meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du6Sbo03ZWsp"
   },
   "source": [
    "# SSSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRD7pbIkoEeJ"
   },
   "source": [
    "### Data (fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLp6Bwl4oJh2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# (training examples, 4 locations, Nfreq, Ntime, 4)\n",
    "data = np.random.rand(50, 4, 501, 69, 4)\n",
    "label = np.random.rand(50, 1)\n",
    "fakeY = np.random.rand(50, 4, 501, 69, 2)\n",
    "for i in range(data.shape[0]):\n",
    "  label[i,0] = random.randint(0, data.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r33cv3jioJkB"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(data)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDKqQFmIZzzP"
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_45KS3DyZ3UE"
   },
   "outputs": [],
   "source": [
    "class mySelfAttention(nn.Module):\n",
    "  def __init__(self, embed_size, heads):\n",
    "      super(mySelfAttention, self).__init__()\n",
    "      self.embed_size = embed_size\n",
    "      self.heads = heads\n",
    "      self.head_dim = embed_size // heads\n",
    "\n",
    "      # assert debug\n",
    "      assert (\n",
    "          self.head_dim * heads == embed_size\n",
    "      ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "      # obtain Q K V matrices by linear transformation\n",
    "      self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "  def forward(self, values, keys, query):\n",
    "      # Get number of training examples\n",
    "      N = query.shape[0]\n",
    "\n",
    "      value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "      # Split the embedding into self.heads different pieces\n",
    "      values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "      keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "      query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "      values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "      keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "      queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "      # Einsum does matrix mult. for query*keys for each training example\n",
    "      # with every other training example, don't be confused by einsum\n",
    "      # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "      energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "      # queries shape: (N, query_len, heads, heads_dim),\n",
    "      # keys shape: (N, key_len, heads, heads_dim)\n",
    "      # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "      # Normalize energy values similarly to seq2seq + attention\n",
    "      # so that they sum to 1. Also divide by scaling factor for\n",
    "      # better stability\n",
    "      attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "      # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "      out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "          N, query_len, self.heads * self.head_dim\n",
    "      )\n",
    "      # attention shape: (N, heads, query_len, key_len)\n",
    "      # values shape: (N, value_len, heads, heads_dim)\n",
    "      # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "      # we reshape and flatten the last two dimensions.\n",
    "\n",
    "      out = self.fc_out(out)\n",
    "      # Linear layer doesn't modify the shape, final shape will be\n",
    "      # (N, query_len, embed_size)\n",
    "      print(out.shape)\n",
    "\n",
    "      return out\n",
    "\n",
    "class myTransformerBlock(nn.Module):\n",
    "  def __init__(self, src_seq_size, heads, dropout, forward_expansion):\n",
    "      super(myTransformerBlock, self).__init__()\n",
    "      self.attention = mySelfAttention(src_seq_size, heads)\n",
    "      self.norm1 = nn.LayerNorm(src_seq_size)\n",
    "      self.norm2 = nn.LayerNorm(src_seq_size)\n",
    "\n",
    "      self.feed_forward = nn.Sequential(\n",
    "          nn.Linear(src_seq_size, forward_expansion * src_seq_size),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(forward_expansion * src_seq_size, src_seq_size),\n",
    "      )\n",
    "\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, value, key, query):\n",
    "      attention = self.attention(value, key, query)\n",
    "\n",
    "      # Add skip connection, run through normalization and finally dropout\n",
    "      x = self.dropout(self.norm1(attention + query))\n",
    "      forward = self.feed_forward(x)\n",
    "      out = self.dropout(self.norm2(forward + x))\n",
    "      return out\n",
    "\n",
    "class myEncoder(nn.Module):\n",
    "  def __init__(self, src_seq_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "    super(myEncoder, self).__init__()\n",
    "    self.embed_size = src_seq_size\n",
    "    self.device = device\n",
    "    self.layers = nn.ModuleList(\n",
    "        [\n",
    "            myTransformerBlock(\n",
    "                src_seq_size,\n",
    "                heads,\n",
    "                dropout=dropout,\n",
    "                forward_expansion=forward_expansion,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, enc_out):\n",
    "    N, seq_length = x.shape\n",
    "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "    out = x\n",
    "\n",
    "    # In the Encoder the query, key, value are all the same, it's in the\n",
    "    # decoder this will change. This might look a bit odd in this case.\n",
    "    for layer in self.layers:\n",
    "        out = layer(out, out, out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class myDecoderBlock(nn.Module):\n",
    "  def __init__(self, src_seq_size, heads, forward_expansion, dropout, device):\n",
    "      super(myDecoderBlock, self).__init__()\n",
    "      self.norm = nn.LayerNorm(src_seq_size)\n",
    "      self.attention = mySelfAttention(src_seq_size, heads=heads)\n",
    "      self.transformer_block = myTransformerBlock(\n",
    "          src_seq_size, heads, dropout, forward_expansion\n",
    "      )\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, value, key):\n",
    "      attention = self.attention(x, x, x, trg_mask)\n",
    "      query = self.dropout(self.norm(attention + x))\n",
    "      out = self.transformer_block(value, key, query, src_mask)\n",
    "      return out\n",
    "\n",
    "class myDecoder(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      trg_vocab_size,\n",
    "      src_seq_size,\n",
    "      num_layers,\n",
    "      heads,\n",
    "      forward_expansion,\n",
    "      dropout,\n",
    "      device,\n",
    "      max_length,\n",
    "  ):\n",
    "      super(myDecoder, self).__init__()\n",
    "      self.device = device\n",
    "\n",
    "      self.layers = nn.ModuleList(\n",
    "          [\n",
    "              DecoderBlock(src_seq_size, heads, forward_expansion, dropout, device)\n",
    "              for _ in range(num_layers)\n",
    "          ]\n",
    "      )\n",
    "      self.fc_out = nn.Linear(src_seq_size, trg_vocab_size)\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "      N, seq_length = x.shape\n",
    "      positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "      x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "      for layer in self.layers:\n",
    "          x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "      out = self.fc_out(x)\n",
    "\n",
    "      return out\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "tEnkkSklipnD",
    "outputId": "a681aa1e-a00a-4726-96c4-9ab82705cdca"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-485978c5e655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(temp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e117cab8130c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, enc_out)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# decoder this will change. This might look a bit odd in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e117cab8130c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, value, key, query)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m       \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;31m# Add skip connection, run through normalization and finally dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e117cab8130c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, values, keys, query)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;31m# Split the embedding into self.heads different pieces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 9, 8, 64]' is invalid for input of size 18"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "  \n",
    "\n",
    "enc = myEncoder(512, 6, 8, 'cuda', 4, 0, 1024)\n",
    "out = enc(x, trg)\n",
    "# print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EpScKTskcu8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SSSL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
